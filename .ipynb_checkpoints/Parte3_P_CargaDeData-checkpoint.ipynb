{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"http://alacip.org/wp-content/uploads/2014/03/logoEscalacip1.png\" width=\"500\"></center>\n",
    "\n",
    "\n",
    "<center> <h1>Curso: Introducción al Python</h1> </center>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "* Profesor:  <a href=\"http://www.pucp.edu.pe/profesor/jose-manuel-magallanes/\" target=\"_blank\">Dr. José Manuel Magallanes, PhD</a> ([jmagallanes@pucp.edu.pe](mailto:jmagallanes@pucp.edu.pe))<br>\n",
    "    - Profesor del **Departamento de Ciencias Sociales, Pontificia Universidad Católica del Peru**.<br>\n",
    "    - Senior Data Scientist del **eScience Institute** and Visiting Professor at **Evans School of Public Policy and Governance, University of Washington**.<br>\n",
    "    - Fellow Catalyst, **Berkeley Initiative for Transparency in Social Sciences, UC Berkeley**.\n",
    "    \n",
    "    \n",
    "## Parte 3:  Carga de datos en Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='beginning'></a>\n",
    "This session pays attention to get data. In this situation, you can be confronted with a decision to collect data from repositories or similar sources, or collect your own data to answer an ad-hoc research question. The latter case will make you consider if you need a probabilistic or non-probabilistic design; which will also determine the next steps in your design.\n",
    "\n",
    "In any case, you need to collect data to be read by R or Python, unless your data is not suitable for any kind of computational data processing. But in this unit, I am assuming it is. If you have collected your data, a popular choice to record your observations is an spreadsheet, maybe using Excel or GoogleDocs. If you have collected data from another party, you may also have spreadsheets, or more sophisticated files in particular formats, like SPSS or STATA. Maybe you decided to collect data from the web, and you may be dealing with XML or JSON formats; or simply text without much structure. Let me show you how to deal with the following cases:\n",
    "\n",
    "1. [Propietary/common software.](#part1) \n",
    "2. [Collecting your own.](#part2) \n",
    "3. [Use of APIs.](#part3) \n",
    "4. [Scraping webpages.](#part4) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the location of your files is extremely important. If you have created a folder name \"my project\", your code should be in that folder, which I call sometimes the root folder,  and your data in another folder inside that root folder. In any case, you should become familiar with some important commands from the **os** package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two more important uses are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# where am I?\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command above gave you your current location, if it is not what you expected, you can change it with another command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to include the path to the folder you want between the parenthesis. \n",
    "\n",
    "Becareful, you need to follow a similar pattern than the one obtained with _os.getcwd()_; that is, see if the folders in the path are using __\\\\__, __\\\\\\__, __/__, __//__ to separate the folders. This difference depends on the type of computer you have. Remember that a path has to be written as a string, that is, in between '' or \"\".\n",
    "\n",
    "You need to change your root folder location once, if needed; but you do not use _ch.dir()_ again for every file you read. If the file is in a folder inside your root folder, you simply write: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder=\"data\"\n",
    "fileName=\"anes_timeseries_2012.dta\"\n",
    "fileToRead=os.path.join(folder,fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object _fileToRead_ has the right name of the path, because **os.path.join** creates a path using the elements between the parenthesis. Notice that if you are using Windows, a folder in \"C\" hard drive should be written like this: \n",
    "os.path.join('c:/','folder1', 'folder2'). Notice that you can write several folders, and path.join creates the right separator, but just for Windows you need that element ':/'. If you want to know the separator your computer is using, type this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.path.sep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn our attention to the file acquisition process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "____\n",
    "\n",
    "\n",
    "<a id='part1'></a>\n",
    "## Collecting data from propietary / common software\n",
    "\n",
    "Let's start with data from STATA, very common in polSci and public policy schools. To work with these kind of files, we will simply use *pandas*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I using a file from the American National Election Studies (ANES). This is a rather big file, so let me select some variables (\"libcpre_self\",\"libcpo_self\",a couple of question pre and post elections asking respondents to place themselves on a seven point scale ranging from ‘extremely liberal’ to ‘extremely conservative’) and create a data frame with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "varsOfInterest=[\"libcpre_self\",\"libcpo_self\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting a Stata file into pandas is quite easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "folder=\"data\"\n",
    "fileName=\"anes_timeseries_2012.dta\"\n",
    "fileToRead=os.path.join(folder,fileName)\n",
    "dataStata=pd.read_stata(fileToRead,columns=varsOfInterest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataStata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting an Excel file is also straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileName=\"ElectricBus.xlsx\"\n",
    "fileToRead=os.path.join(folder,fileName)\n",
    "dataExcel=pd.read_excel(fileToRead,0) # no need for '0'\n",
    "dataExcel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV files are as easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fileName=\"mealSeattle.csv\"\n",
    "fileToRead=os.path.join(folder,fileName)\n",
    "dataCSV=pd.read_csv(fileToRead)\n",
    "dataCSV.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, it is more fun opening several of these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "pattern='interv*.csv'\n",
    "where='data'\n",
    "fileNames = glob(pattern)\n",
    "for filename in glob(os.path.join(where, pattern)):\n",
    "    print (filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had access to the names, the let's make a list of files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles=[]\n",
    "for filename in glob(os.path.join(where, pattern)):\n",
    "    allFiles.append(pd.read_csv(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do we have the data?\n",
    "allFiles[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's concatenate the first 3 files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(allFiles[0:4],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#storing what we did:\n",
    "newOneFile=pd.concat(allFiles[0:4],ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge with last file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newOneFile.merge(allFiles[4],left_on='interview', right_on='interview') # no real need if same keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving the result\n",
    "newOneFile.merge(allFiles[4]).to_csv('data/newOneFile.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to page beginning](#beginning)\n",
    "\n",
    "_____\n",
    "\n",
    "<a id='part2'></a>\n",
    "\n",
    "## Collecting your ad-hoc data\n",
    "\n",
    "Let me assume you are collecting some data using [this](https://goo.gl/forms/f4m4zv41xBh5osrw1) **GoogleForm**. The answers to your form are saved in an spreadsheet, which you should publish as a CSV file. \n",
    "\n",
    "Then, you can read it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "link=''\n",
    "myData = pd.read_csv(link)\n",
    "\n",
    "# here it is:\n",
    "myData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to page beginning](#beginning)\n",
    "\n",
    "-----\n",
    "\n",
    "<a id='part3'></a>\n",
    "\n",
    "## Collecting data from APIs\n",
    "\n",
    "There are organizations, public and private, that have an open data policy that allows people to access their repositories dynamically. You can get that data in CSV format if available, but the data is always in  XML or JSON format, which are containers that store data in an *associative array* structure. Python's dictionaries are very useful in these situations, as they can keep the NOSQL structure better than data frames. Let me get the data about 9-1-1 Police reponses from Seattle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# where is it online?\n",
    "url = \"https://data.seattle.gov/resource/pu5n-trf4.json\"\n",
    "\n",
    "# Go for the data:\n",
    "response = requests.get(url)\n",
    "\n",
    "# If we got the data:\n",
    "if response.status_code == 200:\n",
    "    data911 = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(data911)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You can turn it easily into a pandas data frame:\n",
    "\n",
    "data911DF=pd.DataFrame(data911)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here you are...\n",
    "data911DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The case of Twitter\n",
    "\n",
    "Social media offers rich textual information. In particular, **Twitter** offers an API (registration required) to get the data they have.\n",
    "\n",
    "Follow these steps:\n",
    "1. If you do not have a Twitter account, create one; use your Twitter username and password to access this [link](https://apps.twitter.com/).\n",
    "2. When you are there **Create a new App**. Just complete the basic info requested.\n",
    "3. When the App is created, look for the _Keys and Access Tokens_.\n",
    "4. Open a text editor (a simple one) and create a dictionary like this:  \n",
    "{\"consumer_key\": \"aaa\", \"access_token_secret\": \"bbb\", \"consumer_secret\": \"ccc\", \"access_token\": \"ddd\"}\n",
    "5. Save it as keysAPI.txt, it should be in the root folder for this course (where your codes are).\n",
    "\n",
    "If you did everything ok, the next codes will work:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# get the security info from file\n",
    "keysAPI = json.load(open('data/keysAPI.txt','r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify if you have **tweepy**. You may need to install it via **pip**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "# recovering security info\n",
    "consumer_key = keysAPI['consumer_key']\n",
    "consumer_secret = keysAPI['consumer_secret']\n",
    "access_token = keysAPI['access_token']\n",
    "access_token_secret = keysAPI['access_token_secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using security info:\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api=tweepy.API(auth, wait_on_rate_limit=True,wait_on_rate_limit_notify=True,parser=tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting the tweets from a user:\n",
    "\n",
    "tweets = api.user_timeline(screen_name = 'PepeMujicaDice', count = 100, include_rts = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tweets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aTweet=tweets[0]\n",
    "\n",
    "for field in aTweet.keys():\n",
    "    print (field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aTweet['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aTweet['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the dict into a DF in pandas\n",
    "mujicaTweets=pd.DataFrame({'textTweet':[t['text'] for t in tweets]})\n",
    "mujicaTweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to page beginning](#beginning)\n",
    "\n",
    "_____\n",
    "\n",
    "<a id='part4'></a>\n",
    "\n",
    "## Collecting data by scraping\n",
    "\n",
    "We are going to get the data from a table from this [wikipage](https://en.wikipedia.org/wiki/List_of_freedom_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "# Location \n",
    "wiki=\"https://en.wikipedia.org/wiki/\" \n",
    "link = \"List_of_freedom_indices\" \n",
    "\n",
    "wikiLink=wiki+link\n",
    "# avoid rejection from server\n",
    "identification = {\"User-Agent\":\"Mozilla/5.0\"}\n",
    "# contact server\n",
    "wikiPage =get(wikiLink , headers=identification)\n",
    "# BS gets wikipedia page as html\n",
    "wikiSoup =BS(wikiPage.content ,\"html.parser\")\n",
    "# BS extracts the whole table (it is html) \n",
    "wikiTables=wikiSoup.findAll(\"table\",{\"class\":\"wikitable sortable\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#How many are there?\n",
    "len(wikiTables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# So, I just pick the one I need:\n",
    "wikiTable=wikiTables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# what do you have:\n",
    "wikiTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table is there, but in HTML format. In general, our table is composed of ROWS, so this command will get every row from the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allRows=wikiTable.find_all('tr') #'tr' stands for table row, it is a TAG in HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# headersHtml is simply the first row of the table\n",
    "headersHtml=allRows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and we have:\n",
    "headersHtml # this is ONE element from allRows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just saw the headers, but they are still in HTML; let me use the tag 'th' (table header) to get those elements in _headersHtml_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headersHtml.find_all('th')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see a list of elements, each between the tags 'th' (using < or >). Each element in the list has the text of the header (and other elements). We just need the text, so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# headersList is a list with each headers' TEXT element:\n",
    "headersList=[header.get_text() for header in headersHtml.find_all('th')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Voilà les titres:\n",
    "headersList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same process should be followed to get the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This should be the data:\n",
    "rowsHtml=allRows[1:]  #... [1:] is omitting the Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's see one of these:\n",
    "rowsHtml[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the table cell uses the **td** TAG, but we will not recover ONE row, but several rows, so we need to adapt the previous code for the headers in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some python beauty:\n",
    "# using 'td' \n",
    "rowsList=[[cell.get_text() for cell in row.find_all('td')] for row in rowsHtml]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rowsList[0:3] # a list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data frame creation\n",
    "import pandas as pd\n",
    "\n",
    "# making a data frame from list of lists!\n",
    "pd.DataFrame(data=rowsList , columns=headersList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "**AUSPICIO**: \n",
    "\n",
    "* El desarrollo de estos contenidos ha sido posible gracias al grant del Berkeley Initiative for Transparency in the Social Sciences (BITSS) at the Center for Effective Global Action (CEGA) at the University of California, Berkeley\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.bitss.org/wp-content/uploads/2015/07/bitss-55a55026v1_site_icon.png\" style=\"width: 200px;\"/>\n",
    "</center>\n",
    "\n",
    "* Este curso cuenta con el auspicio de:\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://www.python.org/static/img/psf-logo@2x.png\" style=\"width: 500px;\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "**RECONOCIMIENTO**\n",
    "\n",
    "\n",
    "EL Dr. Magallanes agradece a la Pontificia Universidad Católica del Perú, por su apoyo en la participación en la Escuela ALACIP.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://dci.pucp.edu.pe/wp-content/uploads/2014/02/Logotipo_colores-290x145.jpg\" style=\"width: 400px;\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "El autor reconoce el apoyo que el eScience Institute de la Universidad de Washington le ha brindado desde el 2015 para desarrollar su investigación en Ciencia de Datos.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://escience.washington.edu/wp-content/uploads/2015/10/eScience_Logo_HR.png\" style=\"width: 500px;\"/>\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
